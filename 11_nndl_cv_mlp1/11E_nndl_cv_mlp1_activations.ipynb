{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11E_nndl_cv_mlp1_activations.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"u4hqOKNOwh1v"},"source":["# 11_nndl_cv_mlp1\n","\n","We will experiment with Neural Networks and Deep Learning on the no_na dataset. This is mlp1, our first version of multi-layer perceptron "]},{"cell_type":"code","metadata":{"id":"096C_Ukr5yAS","executionInfo":{"status":"ok","timestamp":1618483253070,"user_tz":-480,"elapsed":2225,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["''' data and math '''\n","import pandas as pd\n","import numpy as np\n","\n","''' plotting images '''\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","''' traversing directories '''\n","import os\n","from pathlib import Path\n","\n","''' utilities '''\n","from tqdm import tqdm\n","\n","''' metrics '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","''' preprocessing '''\n","from sklearn.preprocessing import StandardScaler"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYLxrJY66wVn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618483280655,"user_tz":-480,"elapsed":29779,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"2e0ba7bc-f0f8-48bf-c575-8ad79cb71bcc"},"source":["''' used to reference the root directory, for directory traversal ''' \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","mount_dir = '/content/gdrive'\n","root_dir = Path('/content/gdrive/My Drive/it3011_project')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ABDyA96vaoD"},"source":["# Loading data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sgaXjzb5-Vs","executionInfo":{"status":"ok","timestamp":1618483301346,"user_tz":-480,"elapsed":50456,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"41c58c0b-2e93-4f66-d94f-4266f471ee19"},"source":["# load data\n","train = pd.read_csv(root_dir/\"data/train_no_na.csv\")\n","test = pd.read_csv(root_dir/\"data/test_no_na.csv\")\n","print(\"data loaded\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do5mAxc-0noF","executionInfo":{"status":"ok","timestamp":1618483301348,"user_tz":-480,"elapsed":50445,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"89e442d7-c47b-4f57-ed08-3cdcd42e739e"},"source":["# check shape\n","print(train.shape)\n","print(test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(279331, 138)\n","(120163, 138)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJ1gWmKfaE59","executionInfo":{"status":"ok","timestamp":1618483301350,"user_tz":-480,"elapsed":50401,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"55f21495-b3cf-453b-97e1-7a073c540154"},"source":["# create train/test sets\n","features = [feature for feature in test.keys() if \"feature\" in feature]\n","x_train = train.loc[:, features].values\n","y_train = train.loc[:,['action']].values.flatten()\n","x_test = test.loc[:, features].values\n","y_test = test.loc[:,['action']].values.flatten()\n","print(\"train/test set created\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["train/test set created\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFuaPWdo39ff"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"Ca00riNuRmIa","executionInfo":{"status":"ok","timestamp":1618483301351,"user_tz":-480,"elapsed":50398,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["# constants\n","SEED = 42\n","\n","# cross validation\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=10, random_state=SEED, shuffle=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"P09ivaKS38-l","executionInfo":{"status":"ok","timestamp":1618483301353,"user_tz":-480,"elapsed":50398,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["# create the utility score, which takes in the prediction value and the ground truth action and generates a score\n","# link: https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n","\n","# data: original train/test data    action: the y-value. can either be y_pred or original values too, if we want the max score attainable\n","def utility_score(data, action): \n","  dates_set = set(data.date.values)\n","  dates = data.loc[:, ['date']].values.flatten()\n","  weights = data.loc[:, ['weight']].values.flatten()\n","  resps = data.loc[:, ['resp']].values.flatten()\n","  actions = action.flatten()\n","\n","  i = len(dates_set)\n","  p_i = []\n","\n","  for date in dates_set:\n","    indices = np.where(dates == date)[0]\n","    p_i_temp = 0\n","    for j in indices:\n","      p_i_temp = p_i_temp + weights[j] * resps[j] * actions[j]\n","    p_i.append(p_i_temp)\n","  \n","  p_i_squared = [p_i1*p_i2 for p_i1,p_i2 in zip(p_i,p_i)]\n","  t = ( sum(p_i) / np.sqrt(sum(p_i_squared)) ) * np.sqrt(250/i)\n","  u = min(max(t, 0), 6) * sum(p_i)\n","\n","  return u\n","\n","def max_train_utility_score(data=train, action=y_train):\n","  return utility_score(data, action)\n","\n","def max_test_utility_score(data=test, action=y_test):\n","  return utility_score(data, action)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hflveg2IYc0A","executionInfo":{"status":"ok","timestamp":1618483301355,"user_tz":-480,"elapsed":50397,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["def model_scores(model, test, x_test, y_test):\n","  y_pred = model.predict(x_test) \n","  y_pred = (y_pred > 0.5).astype(int)\n","  \n","  # # get some scores from helpers\n","  utility = utility_score(test, y_pred)\n","  accuracy =  accuracy_score(y_test, y_pred)\n","\n","  # # confusion matrix\n","  # print(\"confusion matrix\")\n","  cm = confusion_matrix(y_test, y_pred)\n","  true_pos = cm[1][1]\n","  true_neg = cm[0][0]\n","  false_pos = cm[0][1]\n","  false_neg = cm[1][0]\n","\n","  # # plot confusion matrix\n","  # fig, ax = plt.subplots(figsize=(3, 3))\n","  # ax.imshow(cm)\n","  # ax.grid(False)\n","  # ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n","  # ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n","  # ax.set_ylim(1.5, -0.5)\n","  # for i in range(2):\n","  #     for j in range(2):\n","  #         ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n","  # plt.show()  \n","\n","  # # AUC-ROC\n","  # print(\"AUC_ROC\")\n","  logit_roc_auc = roc_auc_score(y_test, y_pred)\n","\n","  # # plot auc-roc\n","  # fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n","  # plt.figure()\n","  # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n","  # plt.plot([0, 1], [0, 1],'r--')\n","  # plt.xlim([0.0, 1.0])\n","  # plt.ylim([0.0, 1.05])\n","  # plt.xlabel('False Positive Rate')\n","  # plt.ylabel('True Positive Rate')\n","  # plt.title('Receiver operating characteristic')\n","  # plt.legend(loc=\"lower right\")\n","  # plt.show()\n","\n","  return utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxL5fHUTb8Ba","executionInfo":{"status":"ok","timestamp":1618483301356,"user_tz":-480,"elapsed":50395,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["import datetime\n","import csv\n","\n","def save_scores(output_filename, workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg):\n","  # create output file if not exists\n","  try:\n","    f = open(root_dir/output_filename)\n","  except IOError:\n","    with open (root_dir/output_filename, 'a') as csvfile:\n","      headers = [\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"]\n","      writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers)\n","      writer.writeheader() \n","      print(\"created output file\")  \n","    csvfile.close()\n","\n","  # output file exists, append\n","  timestamp = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n","  \n","  ''' create another df that looks just like the excel file and concat with ''' \n","  new_scores = pd.DataFrame(np.array([[workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg, timestamp]]),\n","                   columns=[\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"],\n","                  )\n","\n","  new_scores.to_csv(root_dir/output_filename, mode='a', header=False, index=False)\n","  print(\"saved model metrics\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg7v_N3iUfwF","executionInfo":{"status":"ok","timestamp":1618483301905,"user_tz":-480,"elapsed":50941,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["'''\n","used to generate the PosixPath variables for the results to save\n","'''\n","def results_paths(root_dir=root_dir):\n","    PATH_RESULTS = root_dir /'results_nndl'\n","    PATH_HISTORIES = PATH_RESULTS / 'histories'\n","    PATH_FIGURES = PATH_RESULTS / 'figures'\n","    PATH_CHECKPOINTS = PATH_RESULTS / 'checkpoints'\n","    PATH_PREDICTIONS = PATH_RESULTS / 'predictions'\n","\n","    return PATH_RESULTS, PATH_HISTORIES, PATH_FIGURES, PATH_CHECKPOINTS, PATH_PREDICTIONS\n","\n","\n","''' \n","used to save the history of a model as a npy file\n","'''\n","# filename like 'history/model_name.npy'\n","def history_saver(history, model_name, history_save_path, already_npy=False):\n","  history_json = {}\n","\n","  if already_npy:\n","    history_npy = history\n","  else:\n","    history_npy = history.history\n","\n","  np.save(history_save_path/model_name, history_npy)\n","  print(\"History saved\")\n","\n","\n","\n","''' \n","used to load the history of a model from a npy file\n","'''\n","# filename like 'history/model_name.npy'\n","def history_loader(model_name, history_save_path):\n","  history_save_path = history_save_path/str(model_name+'.npy')\n","  history=np.load(history_save_path,allow_pickle='TRUE').item()\n","  print('History loaded')\n","  \n","  return history \n","\n","'''\n","used to plot the metrics for a given history\n","'''\n","def plot_metrics(history, model_name, figure_save_path):\n","    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n","\n","    # print(history.keys())\n","\n","    # plot losses\n","    train_loss = history['loss']\n","    val_loss = history['val_loss']\n","    loss_title = 'loss against epochs'\n","\n","    ax1.plot(train_loss, label='train')\n","    ax1.plot(val_loss, label='val')\n","    ax1.set_title(loss_title)\n","    ax1.set_ylabel('loss')\n","    ax1.set_xlabel('epochs')\n","    ax1.legend()\n","\n","    # plot accuracy_score\n","    accuracy_score = history['accuracy']\n","    val_accuracy_score = history['val_accuracy']\n","    accuracy_score_title = 'accuracy_score against epochs'\n","\n","    ax2.plot(accuracy_score, label='train')\n","    ax2.plot(val_accuracy_score, label='val')\n","    ax2.set_title(accuracy_score_title)\n","    ax2.set_ylabel('accuracy_score')\n","    ax2.set_xlabel('epochs')\n","    ax2.legend()\n","\n","\n","    # plot accuracy_score\n","    auc_score = history['auc']\n","    val_auc_score = history['val_auc']\n","    auc_score_title = 'auc_score against epochs'\n","\n","    ax3.plot(auc_score, label='train')\n","    ax3.plot(val_auc_score, label='val')\n","    ax3.set_title(auc_score_title)\n","    ax3.set_ylabel('auc_score')\n","    ax3.set_xlabel('epochs')\n","    ax3.legend()\n","\n","\n","    # save figure\n","    fig.suptitle('Metrics for model: ' + model_name)\n","    plt.savefig(figure_save_path/f'{model_name}.png')\n","\n","    plt.show()  "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwavL2_sPCvc"},"source":["# Neural Network - Model 1 (Activations)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NSmT7RbYSvQ","executionInfo":{"status":"ok","timestamp":1618483305619,"user_tz":-480,"elapsed":54643,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"52c2ee94-693d-4987-a7c2-dc8e4a866c6c"},"source":["!pip install keras_tqdm"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Collecting keras_tqdm\n","  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras_tqdm) (4.41.1)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_tqdm) (2.4.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras_tqdm) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras_tqdm) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras->keras_tqdm) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras_tqdm) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras_tqdm) (1.15.0)\n","Installing collected packages: keras-tqdm\n","Successfully installed keras-tqdm-2.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E0-gAjBEWEjn","executionInfo":{"status":"ok","timestamp":1618483307324,"user_tz":-480,"elapsed":56345,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","from keras_tqdm import TQDMCallback\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\n","from keras.layers.experimental.preprocessing import Normalization\n","from keras.models import Model, Sequential\n","from keras.losses import BinaryCrossentropy\n","from keras.optimizers import Adam\n","from keras.metrics import BinaryAccuracy, AUC\n","\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlejCWbRFGsz","executionInfo":{"status":"ok","timestamp":1618483307325,"user_tz":-480,"elapsed":56343,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["PATH_RESULTS, PATH_HISTORIES, PATH_FIGURES, PATH_CHECKPOINTS, PATH_PREDICTIONS = results_paths()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WVQbc9jVhfu","executionInfo":{"status":"ok","timestamp":1618483307328,"user_tz":-480,"elapsed":56343,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["def create_mlp(num_columns, hidden_units, activation, dropout_rates, learning_rate): \n","  inp = Input(shape=(num_columns,))\n","  x = BatchNormalization()(inp)\n","  x = Dropout(dropout_rates[0])(x)\n","\n","  for i in range(len(hidden_units)):   \n","    x = Dense(hidden_units[i])(x)\n","    x = Activation(activation)(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(dropout_rates[i+1])(x)\n","\n","  x = Dense(1)(x)\n","  out = Activation(\"sigmoid\")(x)\n","\n","  model = Model(inputs=inp, outputs=out)\n","\n","  model.compile(\n","    optimizer=Adam(learning_rate=learning_rate),\n","    loss=BinaryCrossentropy(label_smoothing=1e-4),\n","    metrics=[BinaryAccuracy(name=\"accuracy\"), AUC(name=\"auc\")]\n","  )\n","\n","  return model"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7pLgAALX7vI","executionInfo":{"status":"ok","timestamp":1618483307329,"user_tz":-480,"elapsed":56341,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["# settings to vary\n","\n","# epochs default = [50] going forward for training due to time constraints, final evaluation put about 1000 to try and achieve convergence\n","epochs = [50] \n","\n","# batch_sizes default = [512] going forward for training due to time constraints, final evaluation can be batch size of 128/256 to achieve better utility scores\n","batch_sizes = [512]\n","\n","# hidden_units default = [200, 400, 800, 400] going forward for training due to time constraints, final evaluation can be greater number of hidden units to achieve better utility scores\n","hidden_units = [\n","    [200, 400, 800, 400]\n","]\n","\n","# dropout_rates should have 1 more value than hidden_units\n","# dropout_rates default = [0.2, 0.2, 0.2, 0.2, 0.2] going forward for training due to very poor performance of 0.4, and 0 provides no regularizing effect (model has not converged yet)\n","dropout_rates = [\n","    [0.2, 0.2, 0.2, 0.2, 0.2]\n","] \n","\n","# activations default = [tf.keras.activations.relu]\n","activations = [tf.keras.activations.relu, tf.keras.activations.sigmoid, tf.keras.activations.tanh]\n","\n","# learning_rates default = [1e-4]\n","learning_rates = [1e-4]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"YkWSaZc5Xlwd","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1MRjxULfCpkX93UXnubv9FnUkIopH53WR"},"executionInfo":{"status":"ok","timestamp":1618495021984,"user_tz":-480,"elapsed":11770983,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"e4891221-3a2e-4122-e5fe-b1b70d3ee11d"},"source":["for batch_size in batch_sizes:\n","  for hidden_unit in hidden_units:\n","    for activation in activations:\n","      for dropout_rate in dropout_rates:\n","        for learning_rate in learning_rates:\n","          for epoch in epochs:\n","\n","            output_filename = \"CV_SCORES_AVE_NNDL.csv\"\n","            workbook_name = \"11E_nndl_cv_mlp1_activations\"\n","            model_name = f\"mlp1_activations_{activation}\"\n","            model_params = f\"batch_size={batch_size}, hidden_unit={hidden_unit}, activation={activation}, dropout_rate={dropout_rate}, learning_rate={learning_rate}, epoch={epoch}\"\n","                    \n","            print(\"\")\n","            print(\"model_name: \", model_name)\n","            print(\"model_params: \", model_params)\n","\n","            # cross validation\n","            cv_scores = []\n","\n","            for i, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n","              # train-val data (for utility score calculation) \n","              train_cv, val_cv = train.iloc[train_idx], train.iloc[val_idx]\n","              # train-val features \n","              x_train_cv, x_val_cv, y_train_cv, y_val_cv = x_train[train_idx], x_train[val_idx], y_train[train_idx], y_train[val_idx]\n","\n","              # scaling data to make it easier for models to train\n","              scaler = StandardScaler().fit(x_train_cv)\n","              x_train_cv = scaler.transform(x_train_cv)\n","\n","              # test set scaled on the same scaler as train, because models are fitted on the train distributions and not test distributions\n","              x_val_cv = scaler.transform(x_val_cv)\n","\n","              print(f\"training model fold {i+1}\")\n","\n","              model = create_mlp(x_train_cv.shape[1], hidden_unit, activation, dropout_rate, learning_rate)\n","              model_name_fold = model_name + f\"_fold_{i+1}\"\n","              # CheckpointCallback = ModelCheckpoint(str(PATH_CHECKPOINTS / (model_name + '.hdf5')), monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='auto', save_freq='epoch')\n","\n","              history = model.fit(\n","                  x_train_cv, \n","                  y_train_cv, \n","                  epochs=epoch, \n","                  batch_size=batch_size,\n","                  validation_data=(x_val_cv, y_val_cv),\n","                  # callbacks = [CheckpointCallback]\n","              )\n","\n","              model_score = model_scores(model, test=val_cv, x_test=x_val_cv, y_test=y_val_cv)\n","              cv_scores.append(model_score)\n","\n","              history_saver(history, model_name_fold, PATH_HISTORIES, already_npy=False)\n","              history = history_loader(model_name_fold, PATH_HISTORIES)\n","              plot_metrics(history, model_name_fold, PATH_FIGURES)\n","\n","            # mean of cv scores\n","            cv_scores_ave = [sum(ele) / len(cv_scores) for ele in zip(*cv_scores)]\n","            print(\"cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\")\n","            print(\"cv_scores_ave: \", cv_scores_ave)\n","\n","            # save average scores\n","            save_scores(output_filename, workbook_name, model_name, model_params, *cv_scores_ave)\n"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"oRjGGENMpCDh","executionInfo":{"status":"ok","timestamp":1618495022093,"user_tz":-480,"elapsed":11771088,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["# '''\n","# predict on the test set. load best weights from checkpoints\n","# '''\n","# # model.load_weights(str(PATH_CHECKPOINTS / (model_name + '.hdf5')))\n","\n","# test_metrics = model.evaluate(x_test, steps=3)\n","\n","# test_metrics_dict = {\n","#     'test_loss': test_metrics[0]\n","# }\n","\n","# np.save(PATH_PREDICTIONS/str(model_name + \"_prediction_score\"), test_metrics_dict)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXvTUGFEpnMW","executionInfo":{"status":"ok","timestamp":1618495022106,"user_tz":-480,"elapsed":11771098,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":["# y_pred = (model.predict(x_test) > 0.5).astype(int)\n","# utility_score(test, y_pred)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ha1Hl44pbCz"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"m97QASIApcuw"},"source":[""]},{"cell_type":"code","metadata":{"id":"1H75JYEVVvX_","executionInfo":{"status":"ok","timestamp":1618495022113,"user_tz":-480,"elapsed":11771102,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}}},"source":[""],"execution_count":18,"outputs":[]}]}