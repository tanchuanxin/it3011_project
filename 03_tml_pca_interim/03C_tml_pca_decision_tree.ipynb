{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03C_tml_pca_decision_tree.ipynb","provenance":[],"collapsed_sections":["4ABDyA96vaoD","KFuaPWdo39ff"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u4hqOKNOwh1v"},"source":["# 03_tml_pca\n","\n","We will stack PCA onto the 02 series notebooks to try and reduce the dimensions of our data. We think that not all features are important"]},{"cell_type":"code","metadata":{"id":"096C_Ukr5yAS"},"source":["''' data and math '''\n","import pandas as pd\n","import numpy as np\n","\n","''' plotting images '''\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","''' traversing directories '''\n","import os\n","from pathlib import Path\n","\n","''' utilities '''\n","from tqdm import tqdm\n","\n","''' metrics '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYLxrJY66wVn","executionInfo":{"status":"ok","timestamp":1615477359636,"user_tz":-480,"elapsed":2788658,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"8c38712c-1ab1-4ced-d7df-4fe29a933323"},"source":["''' used to reference the root directory, for directory traversal ''' \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","mount_dir = '/content/gdrive'\n","root_dir = Path('/content/gdrive/My Drive/it3011_project')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ABDyA96vaoD"},"source":["# Loading data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sgaXjzb5-Vs","executionInfo":{"status":"ok","timestamp":1615477396110,"user_tz":-480,"elapsed":2825125,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"4728f8b8-088f-431b-9d9e-bbf8e362ef3f"},"source":["# load data\n","train = pd.read_csv(root_dir/\"data/train.csv\")\n","test = pd.read_csv(root_dir/\"data/test.csv\")\n","print(\"data loaded\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do5mAxc-0noF","executionInfo":{"status":"ok","timestamp":1615477396114,"user_tz":-480,"elapsed":2825120,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"22ccd680-d713-43c6-d40f-e167f3508564"},"source":["# check shape\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(280145, 138)\n","(120504, 138)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJ1gWmKfaE59","executionInfo":{"status":"ok","timestamp":1615477408905,"user_tz":-480,"elapsed":2837904,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"b6c1df64-5a0e-461d-d62c-e72cf53e9c62"},"source":["# create train/test sets\n","features = [feature for feature in test.keys() if \"feature\" in feature]\n","x_train = train.loc[:, features].values\n","y_train = train.loc[:,['action']].values.flatten()\n","x_test = test.loc[:, features].values\n","y_test = test.loc[:,['action']].values.flatten()\n","print(\"train/test set created\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train/test set created\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bCcZ6_5RP3CI"},"source":["# scaling data to make it easier for models to train\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler().fit(x_train)\n","x_train = scaler.transform(x_train)\n","\n","# test set scaled on the same scaler as train, because models are fitted on the train distributions and not test distributions\n","x_test = scaler.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KFuaPWdo39ff"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"Ca00riNuRmIa"},"source":["# constants\n","SEED = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P09ivaKS38-l"},"source":["# create the utility score, which takes in the prediction value and the ground truth action and generates a score\n","# link: https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n","\n","# data: original train/test data    action: the y-value. can either be y_pred or original values too, if we want the max score attainable\n","def utility_score(data, action): \n","  dates_set = set(data.date.values)\n","  dates = data.loc[:, ['date']].values.flatten()\n","  weights = data.loc[:, ['weight']].values.flatten()\n","  resps = data.loc[:, ['resp']].values.flatten()\n","  actions = action.flatten()\n","\n","  i = len(dates_set)\n","  p_i = []\n","\n","  for date in dates_set:\n","    indices = np.where(dates == date)[0]\n","    p_i_temp = 0\n","    for j in indices:\n","      p_i_temp = p_i_temp + weights[j] * resps[j] * actions[j]\n","    p_i.append(p_i_temp)\n","  \n","  p_i_squared = [p_i1*p_i2 for p_i1,p_i2 in zip(p_i,p_i)]\n","  t = ( sum(p_i) / np.sqrt(sum(p_i_squared)) ) * np.sqrt(250/i)\n","  u = min(max(t, 0), 6) * sum(p_i)\n","\n","  return u\n","\n","def max_train_utility_score(data=train, action=y_train):\n","  return utility_score(data, action)\n","\n","def max_test_utility_score(data=test, action=y_test):\n","  return utility_score(data, action)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg7v_N3iUfwF"},"source":["def model_scores(model, test=test, x_test=x_test, y_test=y_test):\n","  y_pred = model.predict(x_test) \n","\n","  print(\"Utility score: \", utility_score(test, y_pred))\n","  print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n","  \n","  print(\"Confusion matrix\")\n","  cm = confusion_matrix(y_test, y_pred)\n","  fig, ax = plt.subplots(figsize=(3, 3))\n","  ax.imshow(cm)\n","  ax.grid(False)\n","  ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n","  ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n","  ax.set_ylim(1.5, -0.5)\n","  for i in range(2):\n","      for j in range(2):\n","          ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n","  plt.show()  \n","\n","  print(\"AUC_ROC\")\n","  logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))\n","  fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n","  plt.figure()\n","  plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n","  plt.plot([0, 1], [0, 1],'r--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver operating characteristic')\n","  plt.legend(loc=\"lower right\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXTMuwYsa7mG"},"source":["# PCA\n","The idea behind PCA is to take the original features and compress them into a smaller set of features, while still keeping as much information as possible. As such, if we are compressing into k features, the 1st feature will retain as much % of the variance represented by the data and so on.\n","\n","The end result is k features, but of note is that the new features have no logical meaning anymore since they are representations of a few features. This is fine as our features are anonymous to begin with - but an important point is that now we have to change our original data to fit the new transformed feature space."]},{"cell_type":"code","metadata":{"id":"HlejCWbRFGsz"},"source":["# import model\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","\n","# settings to vary\n","max_depths = [2,4,8,16,32,64]\n","splitters = ['best', 'random']\n","max_features = [None, 'sqrt', 'log2']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6Xf1Htxu7vc"},"source":["# import model\n","from sklearn import decomposition\n","\n","# settings to vary\n","n_components = [70, 90, 110, 130]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAYe4qKGSImo","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OB_Kj8xAtav7xdJNBYm7Im96x8-4Uryk"},"executionInfo":{"status":"ok","timestamp":1615480813819,"user_tz":-480,"elapsed":6242800,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"3e80f5e3-7a0a-4149-e92d-4250aff15060"},"source":["# iterate over settings for the model\n","for max_depth in max_depths:\n","  for splitter in splitters:\n","    for max_feature in max_features:\n","      for n_component in n_components:\n","        pca = decomposition.PCA(n_components=n_component)\n","        pca.fit(x_train)\n","        x_train_pca = pca.transform(x_train)\n","        x_test_pca = pca.transform(x_test)\n","\n","        print(f\"\\n\\nDecision Tree model: max_depth={max_depth}, splitter={splitter}, max_feature={max_feature}, n_components={n_component}\")    \n","\n","        model = DecisionTreeClassifier(\n","            max_depth=max_depth, \n","            splitter=splitter,\n","            max_features=max_feature\n","        )\n","\n","        model.fit(x_train_pca, y_train)\n","        model_scores(model, x_test=x_test_pca)\n","        # tree.plot_tree(model)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"5Ha1Hl44pbCz"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"m97QASIApcuw"},"source":["Best model with params max_depth = 8, splitter = random, max_feature = none, n_components = 130.\n","\n","* Utility score: 383.076738\n","* Accuracy: 0.5089706566\n","\n","Utility score for decision tree with pca is generally poor, with low to 0 utility scores returned for many models. No clear trend observed and maximum utility score is lower than that of decision tree without PCA."]}]}