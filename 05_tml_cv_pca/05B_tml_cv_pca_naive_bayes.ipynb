{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05B_tml_cv_pca_naive_bayes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"APUKQLQqDnd0"},"source":["# 05_tml_cv_pca\n","\n","We will stack PCA onto the 05 series notebooks to try and reduce the dimensions of our data. We think that not all features are important\n","\n","Following feedback from interim presentation, cross validation will be implemented. That is, we will import the train set, split it into train-val, and run all of our notebooks. Then evaluation on the test set will only be implemented on the selected top models"]},{"cell_type":"code","metadata":{"id":"096C_Ukr5yAS"},"source":["''' data and math '''\n","import pandas as pd\n","import numpy as np\n","\n","''' plotting images '''\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","''' traversing directories '''\n","import os\n","from pathlib import Path\n","\n","''' utilities '''\n","from tqdm import tqdm\n","\n","''' metrics '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","''' preprocessing '''\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYLxrJY66wVn","executionInfo":{"status":"ok","timestamp":1617719154102,"user_tz":-480,"elapsed":2280,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"2b7d801f-b164-4c8d-c268-21da2590fb8c"},"source":["''' used to reference the root directory, for directory traversal ''' \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","mount_dir = '/content/gdrive'\n","root_dir = Path('/content/gdrive/My Drive/it3011_project')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ABDyA96vaoD"},"source":["# Loading data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sgaXjzb5-Vs","executionInfo":{"status":"ok","timestamp":1617719173025,"user_tz":-480,"elapsed":21195,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"7dc8d871-e2b8-443e-dfbe-6647d4115e61"},"source":["# load data\n","train = pd.read_csv(root_dir/\"data/train_no_na.csv\")\n","test = pd.read_csv(root_dir/\"data/test_no_na.csv\")\n","print(\"data loaded\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do5mAxc-0noF","executionInfo":{"status":"ok","timestamp":1617719173026,"user_tz":-480,"elapsed":21189,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"a8a69840-e2e0-4500-eb40-802e867c3f82"},"source":["# check shape\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(279331, 138)\n","(120163, 138)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJ1gWmKfaE59","executionInfo":{"status":"ok","timestamp":1617719173523,"user_tz":-480,"elapsed":21678,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"37519f5a-0467-4962-9735-e47518f0d1e9"},"source":["# create train/test sets\n","features = [feature for feature in test.keys() if \"feature\" in feature]\n","x_train = train.loc[:, features].values\n","y_train = train.loc[:,['action']].values.flatten()\n","x_test = test.loc[:, features].values\n","y_test = test.loc[:,['action']].values.flatten()\n","print(\"train/test set created\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train/test set created\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFuaPWdo39ff"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"Ca00riNuRmIa"},"source":["# constants\n","SEED = 42\n","\n","# cross validation\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=10, random_state=SEED, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P09ivaKS38-l"},"source":["# create the utility score, which takes in the prediction value and the ground truth action and generates a score\n","# link: https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n","\n","# data: original train/test data    action: the y-value. can either be y_pred or original values too, if we want the max score attainable\n","def utility_score(data, action): \n","  dates_set = set(data.date.values)\n","  dates = data.loc[:, ['date']].values.flatten()\n","  weights = data.loc[:, ['weight']].values.flatten()\n","  resps = data.loc[:, ['resp']].values.flatten()\n","  actions = action.flatten()\n","\n","  i = len(dates_set)\n","  p_i = []\n","\n","  for date in dates_set:\n","    indices = np.where(dates == date)[0]\n","    p_i_temp = 0\n","    for j in indices:\n","      p_i_temp = p_i_temp + weights[j] * resps[j] * actions[j]\n","    p_i.append(p_i_temp)\n","  \n","  p_i_squared = [p_i1*p_i2 for p_i1,p_i2 in zip(p_i,p_i)]\n","  t = ( sum(p_i) / np.sqrt(sum(p_i_squared)) ) * np.sqrt(250/i)\n","  u = min(max(t, 0), 6) * sum(p_i)\n","\n","  return u\n","\n","def max_train_utility_score(data=train, action=y_train):\n","  return utility_score(data, action)\n","\n","def max_test_utility_score(data=test, action=y_test):\n","  return utility_score(data, action)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg7v_N3iUfwF"},"source":["def model_scores(model, test, x_test, y_test):\n","  y_pred = model.predict(x_test) \n","  \n","  # # get some scores from helpers\n","  utility = utility_score(test, y_pred)\n","  accuracy =  accuracy_score(y_test, y_pred)\n","\n","  # # confusion matrix\n","  # print(\"confusion matrix\")\n","  cm = confusion_matrix(y_test, y_pred)\n","  true_pos = cm[1][1]\n","  true_neg = cm[0][0]\n","  false_pos = cm[0][1]\n","  false_neg = cm[1][0]\n","\n","  # # plot confusion matrix\n","  # fig, ax = plt.subplots(figsize=(3, 3))\n","  # ax.imshow(cm)\n","  # ax.grid(False)\n","  # ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n","  # ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n","  # ax.set_ylim(1.5, -0.5)\n","  # for i in range(2):\n","  #     for j in range(2):\n","  #         ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n","  # plt.show()  \n","\n","  # # AUC-ROC\n","  # print(\"AUC_ROC\")\n","  logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))\n","\n","  # # plot auc-roc\n","  # fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n","  # plt.figure()\n","  # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n","  # plt.plot([0, 1], [0, 1],'r--')\n","  # plt.xlim([0.0, 1.0])\n","  # plt.ylim([0.0, 1.05])\n","  # plt.xlabel('False Positive Rate')\n","  # plt.ylabel('True Positive Rate')\n","  # plt.title('Receiver operating characteristic')\n","  # plt.legend(loc=\"lower right\")\n","  # plt.show()\n","\n","  return utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSHLF9SGc0gI"},"source":["import datetime\n","import csv\n","\n","def save_scores(output_filename, workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg):\n","  # create output file if not exists\n","  try:\n","    f = open(root_dir/output_filename)\n","  except IOError:\n","    with open (root_dir/output_filename, 'a') as csvfile:\n","      headers = [\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"]\n","      writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers)\n","      writer.writeheader() \n","      print(\"created output file\")  \n","    csvfile.close()\n","\n","  # output file exists, append\n","  timestamp = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n","  \n","  ''' create another df that looks just like the excel file and concat with ''' \n","  new_scores = pd.DataFrame(np.array([[workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg, timestamp]]),\n","                   columns=[\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"],\n","                  )\n","\n","  new_scores.to_csv(root_dir/output_filename, mode='a', header=False, index=False)\n","  print(\"saved model metrics\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXTMuwYsa7mG"},"source":["# PCA\n","The idea behind PCA is to take the original features and compress them into a smaller set of features, while still keeping as much information as possible. As such, if we are compressing into k features, the 1st feature will retain as much % of the variance represented by the data and so on.\n","\n","The end result is k features, but of note is that the new features have no logical meaning anymore since they are representations of a few features. This is fine as our features are anonymous to begin with - but an important point is that now we have to change our original data to fit the new transformed feature space."]},{"cell_type":"code","metadata":{"id":"i6Xf1Htxu7vc"},"source":["# import model\n","from sklearn import decomposition\n","\n","# settings to vary\n","pca_n_components = [70, 90, 110, 130]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cp3JEvUbdRJl"},"source":["# Naive Bayes model"]},{"cell_type":"code","metadata":{"id":"HlejCWbRFGsz"},"source":["# import model\n","from sklearn.naive_bayes import GaussianNB\n","\n","# settings to vary\n","var_smoothings = [1e-0, 1e-03, 1e-06, 1e-09, 1e-12]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HAYe4qKGSImo","executionInfo":{"status":"ok","timestamp":1617719382977,"user_tz":-480,"elapsed":231114,"user":{"displayName":"Chuan Xin Tan","photoUrl":"","userId":"02973160042406904249"}},"outputId":"ea32d4b6-6965-4a00-f698-c1c92239e1de"},"source":["# iterate over settings for the model\n","for pca_n_component in pca_n_components:\n","  pca = decomposition.PCA(n_components=pca_n_component)\n","  pca.fit(x_train)\n","  x_train_pca = pca.transform(x_train)\n","  x_test_pca = pca.transform(x_test)\n","\n","  for var_smoothing in var_smoothings:\n","    output_filename = \"CV_SCORES_AVE_TML.csv\"\n","    workbook_name = \"05B_tml_cv_pca_naive_bayes\"\n","    model_name = \"Naive Bayes model PCA\"\n","    model_params = f\"pca_n_components={pca_n_component}, var_smoothing={var_smoothing}\"\n","    \n","    print(\"\")\n","    print(\"model_name: \", model_name)\n","    print(\"model_params: \", model_params)\n","    \n","    # cross validation\n","    cv_scores = []\n","\n","    for i, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n","      # train-val data (for utility score calculation) \n","      train_cv, val_cv = train.iloc[train_idx], train.iloc[val_idx]\n","      # train-val features \n","      x_train_cv, x_val_cv, y_train_cv, y_val_cv = x_train_pca[train_idx], x_train_pca[val_idx], y_train[train_idx], y_train[val_idx]\n","\n","      # scaling data to make it easier for models to train\n","      scaler = StandardScaler().fit(x_train_cv)\n","      x_train_cv = scaler.transform(x_train_cv)\n","\n","      # test set scaled on the same scaler as train, because models are fitted on the train distributions and not test distributions\n","      x_val_cv = scaler.transform(x_val_cv)\n","\n","      print(f\"training model fold {i+1}\")\n","      model = GaussianNB(var_smoothing=var_smoothing)\n","      model.fit(x_train_cv, y_train_cv)\n","      model_score = model_scores(model, test=val_cv, x_test=x_val_cv, y_test=y_val_cv)\n","      cv_scores.append(model_score)\n","    \n","    # mean of cv scores\n","    cv_scores_ave = [sum(ele) / len(cv_scores) for ele in zip(*cv_scores)]\n","    print(\"cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\")\n","    print(\"cv_scores_ave: \", cv_scores_ave)\n","\n","    # save average scores\n","    save_scores(output_filename, workbook_name, model_name, model_params, *cv_scores_ave)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=70, var_smoothing=1.0\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [84.89285854500625, 0.5225879100680549, 0.5204813159697472, 9046.6, 5550.9, 8129.3, 5206.3]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=70, var_smoothing=0.001\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [64.36133665865643, 0.5063097410382034, 0.5130118996155973, 2651.0, 11491.8, 2188.4, 11601.9]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=70, var_smoothing=1e-06\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.31312220132196, 0.5052071036071695, 0.5121668636238529, 2455.5, 11656.5, 2023.7, 11797.4]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=70, var_smoothing=1e-09\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.28398309163764, 0.5051999436171934, 0.5121599619646224, 2455.2, 11656.6, 2023.6, 11797.7]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=70, var_smoothing=1e-12\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.28398309163764, 0.5051999436171934, 0.5121599619646224, 2455.2, 11656.6, 2023.6, 11797.7]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=90, var_smoothing=1.0\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [86.74988466558105, 0.5227167900157823, 0.5205360732967684, 9100.0, 5501.1, 8179.1, 5152.9]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=90, var_smoothing=0.001\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [75.1945110234851, 0.5104804321234142, 0.5159749711189557, 3552.7, 10706.6, 2973.6, 10700.2]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=90, var_smoothing=1e-06\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.07024416867911, 0.5083360211490735, 0.5140618872820534, 3369.1, 10830.3, 2849.9, 10883.8]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=90, var_smoothing=1e-09\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.113338181181426, 0.5083431811390495, 0.5140692573728068, 3369.0, 10830.6, 2849.6, 10883.9]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=90, var_smoothing=1e-12\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [59.113338181181426, 0.5083431811390495, 0.5140692573728068, 3369.0, 10830.6, 2849.6, 10883.9]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=110, var_smoothing=1.0\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [87.61345082649744, 0.5228313493427624, 0.5206377756686388, 9111.5, 5492.8, 8187.4, 5141.4]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=110, var_smoothing=0.001\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [84.16733698999552, 0.5117871270900605, 0.5168402400530898, 3907.4, 10388.4, 3291.8, 10345.5]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=110, var_smoothing=1e-06\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [50.841146428306125, 0.5061343267946301, 0.5126017279769111, 2883.6, 11254.3, 2425.9, 11369.3]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=110, var_smoothing=1e-09\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [50.685255140467845, 0.5060376675707492, 0.5125127919424728, 2876.5, 11258.7, 2421.5, 11376.4]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=110, var_smoothing=1e-12\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [50.685255140467845, 0.5060376675707492, 0.5125127919424728, 2876.5, 11258.7, 2421.5, 11376.4]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=130, var_smoothing=1.0\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [87.28756879785779, 0.5227955493928823, 0.5205965762717794, 9114.9, 5488.4, 8191.8, 5138.0]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=130, var_smoothing=0.001\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [82.94648823454213, 0.5127107632337853, 0.5173567021288316, 4211.8, 10109.8, 3570.4, 10041.1]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=130, var_smoothing=1e-06\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [45.5917909893472, 0.5089840057527422, 0.5138241335430556, 4108.8, 10108.7, 3571.5, 10144.1]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=130, var_smoothing=1e-09\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [45.58033671602544, 0.5088873457599068, 0.5137502568469065, 4091.1, 10123.7, 3556.5, 10161.8]\n","saved model metrics\n","\n","model_name:  Naive Bayes model PCA\n","model_params:  pca_n_components=130, var_smoothing=1e-12\n","training model fold 1\n","training model fold 2\n","training model fold 3\n","training model fold 4\n","training model fold 5\n","training model fold 6\n","training model fold 7\n","training model fold 8\n","training model fold 9\n","training model fold 10\n","cv_scores key: utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg\n","cv_scores_ave:  [45.58033671602544, 0.5088873457599068, 0.5137502568469065, 4091.1, 10123.7, 3556.5, 10161.8]\n","saved model metrics\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Ha1Hl44pbCz"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"m97QASIApcuw"},"source":["Best performing hyperparameters: pca_n_components=110, var_smoothing=1.0\n","* Utility score: 87.61345083\n","* Accuracy: 0.522831349\n","\n","Much like 4B Naive Bayes without PCA, observable trend of increased utility score and accuracy with increase in var_smoothing. Also observed that pca_components = 110 performs the best."]}]}