{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_tml_test_best_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"APUKQLQqDnd0"},"source":["# 06_tml_test\n","\n","From the 04/05 series of notebooks, we have used 10-fold cross validation to try and determine the optimal model, and its set of hyperparameters for this problem. \n","\n","Now, we will evaluate the top model on the test set and select the winner. We also run a pure RNG as a baseline comparison"]},{"cell_type":"code","metadata":{"id":"096C_Ukr5yAS"},"source":["''' data and math '''\n","import pandas as pd\n","import numpy as np\n","\n","''' plotting images '''\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","''' traversing directories '''\n","import os\n","from pathlib import Path\n","\n","''' utilities '''\n","from tqdm import tqdm\n","\n","''' metrics '''\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","''' preprocessing '''\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYLxrJY66wVn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618421672420,"user_tz":-480,"elapsed":23250,"user":{"displayName":"Shaun Chua","photoUrl":"","userId":"16478631659259129634"}},"outputId":"8e0546c8-2cd4-4217-f6f9-5965ff5974c8"},"source":["''' used to reference the root directory, for directory traversal ''' \n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","mount_dir = '/content/gdrive'\n","root_dir = Path('/content/gdrive/My Drive/it3011_project')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4ABDyA96vaoD"},"source":["# Loading data"]},{"cell_type":"code","metadata":{"id":"0sgaXjzb5-Vs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618421712174,"user_tz":-480,"elapsed":62994,"user":{"displayName":"Shaun Chua","photoUrl":"","userId":"16478631659259129634"}},"outputId":"6dfae51e-b913-4244-8ba5-73e5e6baa835"},"source":["# load data\n","train = pd.read_csv(root_dir/\"data/train_no_na.csv\")\n","test = pd.read_csv(root_dir/\"data/test_no_na.csv\")\n","print(\"data loaded\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"do5mAxc-0noF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618421712176,"user_tz":-480,"elapsed":62987,"user":{"displayName":"Shaun Chua","photoUrl":"","userId":"16478631659259129634"}},"outputId":"80bba048-6f26-428b-df8d-47096ea4ebd5"},"source":["# check shape\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(279331, 138)\n","(120163, 138)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aJ1gWmKfaE59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618421712634,"user_tz":-480,"elapsed":63436,"user":{"displayName":"Shaun Chua","photoUrl":"","userId":"16478631659259129634"}},"outputId":"67f035f3-993d-4bef-b20e-4482ab61485e"},"source":["# create train/val/test sets\n","features = [feature for feature in test.keys() if \"feature\" in feature]\n","x_train = train.loc[:, features].values\n","y_train = train.loc[:,['action']].values.flatten()\n","x_test = test.loc[:, features].values\n","y_test = test.loc[:,['action']].values.flatten()\n","print(\"train/test set created\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train/test set created\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFuaPWdo39ff"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"Ca00riNuRmIa"},"source":["# constants\n","SEED = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P09ivaKS38-l"},"source":["# create the utility score, which takes in the prediction value and the ground truth action and generates a score\n","# link: https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n","\n","# data: original train/test data    action: the y-value. can either be y_pred or original values too, if we want the max score attainable\n","def utility_score(data, action): \n","  dates_set = set(data.date.values)\n","  dates = data.loc[:, ['date']].values.flatten()\n","  weights = data.loc[:, ['weight']].values.flatten()\n","  resps = data.loc[:, ['resp']].values.flatten()\n","  actions = action.flatten()\n","\n","  i = len(dates_set)\n","  p_i = []\n","\n","  for date in dates_set:\n","    indices = np.where(dates == date)[0]\n","    p_i_temp = 0\n","    for j in indices:\n","      p_i_temp = p_i_temp + weights[j] * resps[j] * actions[j]\n","    p_i.append(p_i_temp)\n","  \n","  p_i_squared = [p_i1*p_i2 for p_i1,p_i2 in zip(p_i,p_i)]\n","  t = ( sum(p_i) / np.sqrt(sum(p_i_squared)) ) * np.sqrt(250/i)\n","  u = min(max(t, 0), 6) * sum(p_i)\n","\n","  return u\n","\n","def max_train_utility_score(data=train, action=y_train):\n","  return utility_score(data, action)\n","\n","def max_test_utility_score(data=test, action=y_test):\n","  return utility_score(data, action)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg7v_N3iUfwF"},"source":["def model_scores(model, test, x_test, y_test):\n","  y_pred = model.predict(x_test) \n","  \n","  # # get some scores from helpers\n","  utility = utility_score(test, y_pred)\n","  accuracy =  accuracy_score(y_test, y_pred)\n","\n","  # # confusion matrix\n","  # print(\"confusion matrix\")\n","  cm = confusion_matrix(y_test, y_pred)\n","  true_pos = cm[1][1]\n","  true_neg = cm[0][0]\n","  false_pos = cm[0][1]\n","  false_neg = cm[1][0]\n","\n","  # # plot confusion matrix\n","  # fig, ax = plt.subplots(figsize=(3, 3))\n","  # ax.imshow(cm)\n","  # ax.grid(False)\n","  # ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n","  # ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n","  # ax.set_ylim(1.5, -0.5)\n","  # for i in range(2):\n","  #     for j in range(2):\n","  #         ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n","  # plt.show()  \n","\n","  # # AUC-ROC\n","  # print(\"AUC_ROC\")\n","  logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))\n","\n","  # # plot auc-roc\n","  # fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n","  # plt.figure()\n","  # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n","  # plt.plot([0, 1], [0, 1],'r--')\n","  # plt.xlim([0.0, 1.0])\n","  # plt.ylim([0.0, 1.05])\n","  # plt.xlabel('False Positive Rate')\n","  # plt.ylabel('True Positive Rate')\n","  # plt.title('Receiver operating characteristic')\n","  # plt.legend(loc=\"lower right\")\n","  # plt.show()\n","\n","  return utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpcZgPkPJDvf"},"source":["import datetime\n","import csv\n","\n","def save_scores(output_filename, workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg):\n","  # create output file if not exists\n","  try:\n","    f = open(root_dir/output_filename)\n","  except IOError:\n","    with open (root_dir/output_filename, 'a') as csvfile:\n","      headers = [\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"]\n","      writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers)\n","      writer.writeheader() \n","      print(\"created output file\")  \n","    csvfile.close()\n","\n","  # output file exists, append\n","  timestamp = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n","  \n","  ''' create another df that looks just like the excel file and concat with ''' \n","  new_scores = pd.DataFrame(np.array([[workbook_name, model_name, model_params, utility, accuracy, logit_roc_auc, true_pos, true_neg, false_pos, false_neg, timestamp]]),\n","                   columns=[\"workbook_name\", \"model_name\", \"model_params\", \"utility\", \"accuracy\", \"logit_roc_auc\", \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\", \"timestamp\"],\n","                  )\n","\n","  new_scores.to_csv(root_dir/output_filename, mode='a', header=False, index=False)\n","  print(\"saved model metrics\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrwUKgT48lD8"},"source":["# #1 Random Forest, no PCA, n_estimators=50, max_depth=16, max_feature=None"]},{"cell_type":"code","metadata":{"id":"HlejCWbRFGsz"},"source":["# import model\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# settings to vary\n","n_estimators = [50]\n","max_depths = [16]\n","max_features = [None]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAYe4qKGSImo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618424807445,"user_tz":-480,"elapsed":3158225,"user":{"displayName":"Shaun Chua","photoUrl":"","userId":"16478631659259129634"}},"outputId":"4edd9a75-e7a2-4bc3-fc52-bedef86d06e0"},"source":["# iterate over settings for the model\n","for n_estimator in n_estimators:\n","  for max_depth in max_depths:\n","    for max_feature in max_features:\n","      output_filename = \"TEST_SCORES.csv\"\n","      workbook_name = \"06_tml_test_best_model\"\n","      model_name = \"Best Model - Random Forest no PCA\"\n","      model_params = f\"n_estimators={n_estimator}, max_depth={max_depth}, max_feature={max_feature}\"\n","      \n","      print(\"\")\n","      print(\"model_name: \", model_name)\n","      print(\"model_params: \", model_params)\n","      \n","      # scaling data to make it easier for models to train\n","      scaler = StandardScaler().fit(x_train)\n","      x_train = scaler.transform(x_train)\n","\n","      # test set scaled on the same scaler as train, because models are fitted on the train distributions and not test distributions\n","      x_test = scaler.transform(x_test)\n","\n","      print(f\"training model\")\n","      model = RandomForestClassifier(\n","        n_estimators=n_estimator, \n","        max_depth=max_depth,\n","        max_features=max_feature\n","      )\n","      model.fit(x_train, y_train)\n","      model_score = model_scores(model, test=test, x_test=x_test, y_test=y_test)\n","      \n","      # save average scores\n","      save_scores(output_filename, workbook_name, model_name, model_params, *model_score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","model_name:  Best Model - Random Forest no PCA\n","model_params:  n_estimators=50, max_depth=16, max_feature=None\n","training model\n","saved model metrics\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Ha1Hl44pbCz"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"m97QASIApcuw"},"source":["* Utility score: 1599.608341\n","* Accuracy: 0.51332773\n","\n","Good job, we almost doubled the utility score versus the RNG set!"]},{"cell_type":"code","metadata":{"id":"eYxCWS8ZcvQu"},"source":[""],"execution_count":null,"outputs":[]}]}